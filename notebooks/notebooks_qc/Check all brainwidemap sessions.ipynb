{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibl_pipeline import ephys, acquisition, subject, data\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load eids of all brainwidemap_sessions\n",
    "with open('notebooks_qc/brainwidemap_sessions.txt', 'r') as file:\n",
    "    eids_raw = file.readlines()\n",
    "\n",
    "eids = {eid.replace('\\n', '') for eid in eids_raw}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether required datasets are missing\n",
    "required_datasets = ephys.CompleteClusterSession.required_datasets\n",
    "required_datasets\n",
    "\n",
    "# sessions with complete datasets, but DefaultCluster data are missing\n",
    "missing_sessions_with_complete_datasets = []\n",
    "missing_sessions_with_incomplete_datasets = dict()\n",
    "for eid in tqdm(eids):\n",
    "    key = acquisition.Session & {'session_uuid': eid}\n",
    "    if not ephys.DefaultCluster & key:\n",
    "        missing_datasets = [\n",
    "            dataset for dataset in required_datasets \n",
    "            if not data.FileRecord & key & {'dataset_name': dataset} & 'repo_name LIKE \"flatiron_%\"' & {'exists': 1}]\n",
    "        if missing_datasets:\n",
    "            missing_sessions_with_incomplete_datasets[eid] = missing_datasets\n",
    "        else:\n",
    "            missing_sessions_with_complete_datasets.append(eid)\n",
    "\n",
    "print(f'Number of sessions with complete datasets but missing cluster data: \\\n",
    "      {len(missing_sessions_with_complete_datasets)}')\n",
    "\n",
    "print(f'Number of sessions with incomplete datasets: \\\n",
    "      {len(missing_sessions_with_incomplete_datasets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('missing_sessions', missing_sessions_with_complete_datasets, missing_sessions_with_incomplete_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "eids = np.load('missing_sessions.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.DefaultCluster.populate(\n",
    "    acquisition.Session & [{'session_uuid': eid} for eid in eids], display_progress=True, suppress_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check with ONE whether these missing datasets are really missing\n",
    "from oneibl.one import ONE\n",
    "one = ONE()\n",
    "\n",
    "for eid in tqdm(missing_sessions_with_incomplete_datasets.keys()):\n",
    "    datasets = one.alyx.rest('datasets', 'list', session=eid)\n",
    "    for d in datasets:\n",
    "        if d['name'] in missing_sessions_with_incomplete_datasets[eid]:\n",
    "            print(f'File {d[\"name\"]} exists for session {eid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alf.io\n",
    "\n",
    "# check data length consistency of the 17 sessions\n",
    "\n",
    "ephys_dtypes = [\n",
    "    'clusters.amps',\n",
    "    'clusters.channels',\n",
    "    'clusters.depths',\n",
    "    'clusters.metrics',\n",
    "    'clusters.peakToTrough',\n",
    "    'clusters.uuids',\n",
    "    'clusters.waveforms',\n",
    "    'clusters.waveformsChannels',\n",
    "    'spikes.amps',\n",
    "    'spikes.clusters',\n",
    "    'spikes.depths',\n",
    "    'spikes.samples',\n",
    "    'spikes.templates'\n",
    "]\n",
    "for eid in missing_sessions_with_complete_datasets[0:1]:\n",
    "    \n",
    "    session_key = acquisition.Session & {'session_uuid': eid}\n",
    "    probe_keys = (ephys.ProbeInsertion & session_key).fetch('KEY')\n",
    "    \n",
    "    for key in probe_keys:\n",
    "        # load relevant data\n",
    "        spikes_times_dtype_name = (\n",
    "            data.FileRecord & key &\n",
    "            'dataset_name like \"%spikes.times%.npy\"').fetch1(\n",
    "                'dataset_name').split('.npy')[0]\n",
    "        dtypes = ephys_dtypes + [spikes_times_dtype_name]\n",
    "        \n",
    "        files = one.load(eid, dataset_types=dtypes, download_only=True,\n",
    "                         clobber=True)\n",
    "        ses_path = alf.io.get_session_path(files[0])\n",
    "\n",
    "        probe_name = (ephys.ProbeInsertion & key).fetch1('probe_label')\n",
    "\n",
    "        clusters = alf.io.load_object(\n",
    "            ses_path.joinpath('alf', probe_name), 'clusters')\n",
    "        spikes = alf.io.load_object(\n",
    "            ses_path.joinpath('alf', probe_name), 'spikes')\n",
    "\n",
    "        time_fnames = [k for k in spikes.keys() if 'times' in k]\n",
    "\n",
    "        \n",
    "        # check clusters.* data, report if length does not match the length of clusters.uuids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what's happening with the 17 sessions\n",
    "ephys.DefaultCluster.populate(acquisition.Session & [{'session_uuid': eid} for eid in missing_sessions_with_complete_datasets], display_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alf.io.load_object(ses_path.joinpath('alf', probe_name), object='spikes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "dj.U('session_uuid') & (acquisition.Session & ephys.DefaultCluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eid = 'f354dc45-caef-4e3e-bd42-2c19a5425114'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_key = acquisition.Session & {'session_uuid': eid}\n",
    "probe_keys = (ephys.ProbeInsertion & session_key).fetch('KEY')\n",
    "\n",
    "for key in probe_keys:\n",
    "    # load relevant data\n",
    "    spikes_times_dtype_name = (\n",
    "        data.FileRecord & key &\n",
    "        'dataset_name like \"%spikes.times.npy\"').fetch1(\n",
    "            'dataset_name').split('.npy')[0]\n",
    "    dtypes = ephys_dtypes + [spikes_times_dtype_name]\n",
    "\n",
    "    files = one.load(eid, dataset_types=dtypes, download_only=True,\n",
    "                     clobber=True)\n",
    "    ses_path = alf.io.get_session_path(files[0])\n",
    "\n",
    "    probe_name = (ephys.ProbeInsertion & key).fetch1('probe_label')\n",
    "\n",
    "    clusters = alf.io.load_object(\n",
    "        ses_path.joinpath('alf', probe_name), 'clusters')\n",
    "    spikes = alf.io.load_object(\n",
    "        ses_path.joinpath('alf', probe_name), 'spikes')\n",
    "\n",
    "    time_fnames = [k for k in spikes.keys() if 'times' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibl_pipeline import subject, acquisition, ephys, behavior, data\n",
    "from ibl_pipeline.plotting import ephys as ephys_plotting\n",
    "from ibl_pipeline.group_shared import wheel\n",
    "import datajoint as dj\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = (acquisition.Session & ephys.DefaultCluster).fetch('KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys_plotting.Raster.populate(display_progress=True, suppress_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.conn().connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
