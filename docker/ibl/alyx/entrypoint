#! /bin/bash

# IBL Alyx Entrypoint Script
# ==========================

err_exit() {
	echo "#! Error: $*"
	exit 1
}

# globals
# -------

# get this script file and its parent directory name (no symlink resolution)
script_cmd="${BASH_SOURCE[0]}"
script_path="$(cd "$(dirname ${script_cmd})" &>/dev/null && pwd)"
script_file=$(basename "${script_cmd}")

# use default dockerfile directory if root path not specified
[ -z "${IBL_PATH_ROOT}" ] && IBL_PATH_ROOT=/int-brain-lab

# get alyx source code directory or default to ./alyx
[ -z "${IBL_PATH_ALYX}" ] && IBL_PATH_ALYX=/var/www/alyx

# get data directory or default to ./data
[ -z "${IBL_PATH_DATA}" ] && IBL_PATH_DATA="${IBL_PATH_ROOT}/data"

# get default port for alyx server
[ -z "${ALYX_PORT}" ] && ALYX_PORT=8000

# get current date as default
[ -z "${ALYX_DL_DATE}" ] && ALYX_DL_DATE=$(date -u +'%Y-%m-%d')

# paths for alyx files used for db setup steps
alyxmanagepy="${IBL_PATH_ALYX}/alyx/manage.py"
dbloaded="${IBL_PATH_DATA}/alyx/db_loaded"
sucreated="${IBL_PATH_DATA}/alyx/superuser_created"

# dump variables
ALYX_DUMP_URL=
ALYX_DUMP_FILE=
ALYX_DUMP_EXPIRES=6
ALYX_DUMP_DIR="${IBL_PATH_DATA}/alyx/dumps"

# help documentation
# ------------------

show_help() {
	echo "usage: $script_file FUNC/ROUTINE... [OPTIONS]... [-- CMD]

Entrypoint for Alyx database management


Options:

--dump_date=ALYX_DL_DATE ...... Date used to fetch database dump in YYYY-mm-dd format
--dump_url=ALYX_DUMP_URL ...... Full url of sql dump instead of using ALYX_DL_DATE
                                to form the url
--dump_file=ALYX_DUMP_FILE .... Path to store or load local copy of SQL dump, defaults
                                to file formed via ALYX_DL_DATE
--dump_exp=ALYX_DUMP_EXPIRES .. Number of days for dumps to be considered expired when
                                using function 'cleandumps'. Default is ${ALYX_DUMP_EXPIRES} days.


Functions:

fetchdump ..... Download Alyx database dump from the date set in env variable ALYX_DL_DATE
                or from the option --dump_url.
mkdb .......... Create new, empty '${PGDBNAME}' and '${PGDBNAME}_old' postgres databases
                if they do not already exist.
alyxcfg ....... Configure Alyx/django settings. The environment variables
                PGUSER PGHOST and PGPASSWORD are required.
                Run manage.py w/ 'makemigrations' and 'migrate' then create superuser.
loaddb ........ Load data from the sql dump into postgres '${PGDBNAME}' db if
                file '$dbloaded' does not already exist
alyxstart ..... Start Alyx server using port $ALYX_PORT
renamedb ...... Switch '${PGDBNAME}' to '${PGDBNAME}_old' and create a new empty '${PGDBNAME}' in postgres
dumpjson....... Send request to Alyx to perform a JSON dump of the current database
cleandumps .... Clean up existing sql dumps in sql dump folder. Use with option '--dump_exp'.
fetchpubdump .. Download the public Alyx database dump from flatiron.


Routines:

initdb ........ Routine that performs all initialization steps to bring up the
                '${PGDBNAME}' database by running: mkdb, alyxcfg, alyxmigrate
loaddump ...... Routine that inserts the sql dump into the '${PGDBNAME}' postgres database by
                running: fetchdump, mkdb, alyxcfg, alyxmigrate, loaddb
reloaddb ...... Routine that fetches a new dump from ALYX_DL_DATE and force reload into
                '${PGDBNAME}' db by running: fetchdump, loaddb
www ........... Routine that initializes environment and runs Alyx web
                by running: fetchdump, mkdb, alyxcfg, alyxmigrate, loaddb, alyxstart
dev ........... Initialize parent process and wait indefinitely


Environment Variables:

  IBL_PATH_ROOT=$IBL_PATH_ROOT
  IBL_PATH_ALYX=$IBL_PATH_ALYX
  IBL_PATH_DATA=$IBL_PATH_DATA
  PGDBNAME=$PGDBNAME
  PGUSER=$PGUSER
  PGHOST=$PGHOST
  ALYX_DL_DATE=$ALYX_DL_DATE
  ALYX_DUMP_FILE=$ALYX_DUMP_FILE
  ALYX_DUMP_DIR=$ALYX_DUMP_DIR
  ALYX_PORT=$ALYX_PORT


Database File Status:

  db_loaded: $(test -f $dbloaded && echo "YES" || echo "NO")
  superuser_created: $(test -f $sucreated && echo "YES" || echo "NO")


Examples:

 Run operations in a specific order

   $script_file fetchdump mkdb loaddb

 Fetch and load dump from specific date

   $script_file --dump_date=2021-10-07 reloaddb

 Load public data dump

   $script_file --dump_file=/int-brain-lab/shared/public_alyx.sql.gz loaddump

 Clean existing sql dumps older than 3 days

   $script_file --dump_exp=3 cleandumps

 Bypass routines and run a command

   $script_file -- echo 'hello'
"
	exit 0
}

# flag to show help after some variables are populated
SHOW_HELP=0

# stores set of db functions to run in sequence
OPS_SEQ=()

# handle input arguments
while [[ $# -gt 0 ]]; do
	case "$1" in
	--dump_date=*)
		ALYX_DL_DATE="${1#*=}"
		echo "--dump_date passed. Using dump date: '$ALYX_DL_DATE'"
		shift
		;;
	--dump_url=*)
		ALYX_DUMP_URL="${1#*=}"
		echo "--dump_url passed. Using dump url: '$ALYX_DUMP_URL'"
		shift
		;;
	--dump_file=*)
		export ALYX_DUMP_FILE="${1#*=}"
		echo "--dump_file passed. Using dump file: '$ALYX_DUMP_FILE'"
		shift
		;;
	--dump_exp=*)
		ALYX_DUMP_EXPIRES="${1#*=}"
		echo "--dump_exp passed. Using dump expiration: '$ALYX_DUMP_EXPIRES' days"
		shift
		;;
	"help" | "--help" | "-h" | "--help")
		SHOW_HELP=1
		break
		;;
	"dev")
		OPS_SEQ+=("dev")
		shift
		;;
	"fetchdump")
		OPS_SEQ+=("fetchdump")
		shift
		;;
	"fetchpubdump")
		OPS_SEQ+=("fetchdump")
		shift
		;;
	"mkdb")
		OPS_SEQ+=("mkdb")
		shift
		;;
	"alyxcfg")
		OPS_SEQ+=("alyxcfg")
		shift
		;;
	"alyxmigrate")
		OPS_SEQ+=("alyxmigrate")
		shift
		;;
	"alyxstart")
		OPS_SEQ+=("alyxstart")
		shift
		;;
	"loaddb")
		OPS_SEQ+=("loaddb")
		shift
		;;
	"renamedb")
		OPS_SEQ+=("renamedb")
		shift
		;;
	"dumpjson")
		OPS_SEQ+=("dumpjson")
		shift
		;;
	"cleandumps")
		OPS_SEQ+=("cleandumps")
		shift
		;;
	"initdb")
		OPS_SEQ+=("initdb")
		shift
		;;
	"loaddump")
		OPS_SEQ+=("loaddump")
		shift
		;;
	"www")
		OPS_SEQ+=("www")
		shift
		;;
	"reloaddb")
		OPS_SEQ+=("reloaddb")
		shift
		;;
	"--")
		shift
		echo "Running command: $*"
		break
		;;
	*)
		err_exit "Unknown option: $1"
		;;
	esac
done

if [[ -z "${ALYX_DUMP_FILE}" ]]; then
	# where to save/load sql dump
	export ALYX_DUMP_FILE="${ALYX_DUMP_DIR}/sql/${ALYX_DL_DATE}_alyxfull.sql.gz"
fi

# show help if asked
# ------------------

[ ${SHOW_HELP} -eq 1 ] && show_help

# other checks
# ------------

# check that IBL path is a directory that exists
[ -d "${IBL_PATH_ROOT}" ] ||
	err_exit "path '${IBL_PATH_ROOT}' from IBL_PATH_ROOT does not exist"

# check that IBL alyx path is a directory that exists
[ -d "${IBL_PATH_ALYX}" ] ||
	err_exit "path '${IBL_PATH_ALYX}' from IBL_PATH_ALYX does not exist"

# check that IBL data path is a directory that exists
[ -d "${IBL_PATH_DATA}" ] ||
	err_exit "path '${IBL_PATH_DATA}' from IBL_PATH_DATA does not exist"

# check for proper alyx source code directory
[ -f "${alyxmanagepy}" ] ||
	err_exit "can't locate '$alyxmanagepy'; check alyx source code path IBL_PATH_ALYX='$IBL_PATH_ALYX'"

[ -z "$PGDBNAME" ] && err_exit "PGDBNAME database name environment variable is not set"

# make directories if they don't exist
[ -d "${ALYX_DUMP_DIR}/sql" ] || mkdir -p "${ALYX_DUMP_DIR}/sql"
[ -d "${ALYX_DUMP_DIR}/json" ] || mkdir -p "${ALYX_DUMP_DIR}/json"
[ -d "${ALYX_DUMP_DIR}/backup" ] || mkdir -p "${ALYX_DUMP_DIR}/backup"

# helpers
# -------

checkdb() {
	local datname="$1"
	[[ -z "${datname}" ]] && datname="${PGDBNAME}"
	echo "# => checking for existing database '${datname}'"
	DB_NAME_EXISTS=0
	[[ $(psql -tAc "SELECT 1 FROM pg_database WHERE datname='${datname}'") = "1" ]] && DB_NAME_EXISTS=1
}

checkdbusr() {
	local usename="$1"
	[[ -z "${usename}" ]] && usename="${PGUSER}"
	echo "# => checking for existing database user '${usename}'"
	DB_USER_EXISTS=0
	[[ $(psql -tAc "SELECT 1 FROM pg_user WHERE usename  = '${usename}'") = "1" ]] && DB_USER_EXISTS=1
}

checkurl() {
	local chkurl="$1"
	shift
	[[ -z "${chkurl}" ]] && err_exit "checkurl() requires at least 1 input arg"
	echo "=> checking for a valid flatiron URL: $chkurl"
	local response=$(wget --server-response --spider $chkurl $* 2>&1)
	CONNECTION_ESTABLISHED=1
	if [[ ! $(echo "$response" 2>&1 | grep 'HTTP/1.1 200 OK') ]] ||
		[[ ! $(echo "$response" 2>&1 | grep 'Connection: Keep-Alive') ]]; then
		echo "Warning: could not connect to: '$chkurl' w/ '$*'"
		echo "$response"
		CONNECTION_ESTABLISHED=0
	fi
}

alyxcmd() {
	local acmd="$1"
	shift
	[[ -z "${acmd}" ]] && acmd=help
	python "${alyxmanagepy}" "${acmd}" $*
}

# functions for postgres/django operations
# ----------------------------------------

mkdb() {
	checkdb "${PGDBNAME}"
	if [[ ${DB_NAME_EXISTS} -eq 0 ]]; then
		echo "# ==> creating database ${PGDBNAME}"
		psql -c "CREATE DATABASE ${PGDBNAME}"
	fi

	checkdb "${PGDBNAME}_old"
	if [[ ${DB_NAME_EXISTS} -eq 0 ]]; then
		echo "# ==> creating database ${PGDBNAME}_old"
		psql -c "CREATE DATABASE ${PGDBNAME}_old"
	fi
}

fetchdump() {
	if [[ -f "${ALYX_DUMP_FILE}" ]]; then
		echo "alyx dump file '$ALYX_DUMP_FILE' already exists"
		echo "run 'rm -f $ALYX_DUMP_FILE' to forcibly remove if you want to re-download"
		return 0
	fi

	[[ -z "${HTTP_DATA_SERVER_LOGIN}" ]] && err_exit "HTTP_DATA_SERVER_LOGIN environment variable is not set"
	[[ -z "${HTTP_DATA_SERVER_PWD}" ]] && err_exit "HTTP_DATA_SERVER_PWD environment variable is not set"

	local base_url="http://ibl.flatironinstitute.org/json"
	local url="${ALYX_DUMP_URL}"
	[[ -z "${url}" ]] && url="${base_url}/${ALYX_DL_DATE}_alyxfull.sql.gz"

	checkurl "$url" --user "${HTTP_DATA_SERVER_LOGIN}" --password "${HTTP_DATA_SERVER_PWD}"
	if [[ $CONNECTION_ESTABLISHED -eq 0 ]]; then
		unset ALYX_DUMP_FILE
		return 0
	fi

	mkdir -p $(dirname "${ALYX_DUMP_FILE}")
	echo "==> downloading database dump from '$url' ..."
	wget -q -O ${ALYX_DUMP_FILE} --user "$HTTP_DATA_SERVER_LOGIN" --password "$HTTP_DATA_SERVER_PWD" "${url}"
	echo "===> download complete, saved to file: '$ALYX_DUMP_FILE'"
}

fetchpubdump() {
	local pub_file=public_alyx.sql.gz
	local pub_url=http://ibl.flatironinstitute.org/json/${pub_file}
	local pub="${IBL_PATH_ROOT}/shared/${pub_file}"
	if [ -f "${pub}" ]; then
		echo "public dump file '${pub}' already exists"
		echo "run 'rm -f ${pub}' to forcibly remove if you want to re-download"
		return 0
	fi
	mkdir -p $(dirname "${pub}")
	echo "=> downloading public database dump from '$pub_url' ..."
	wget -q -O "${pub}" --user "$HTTP_DATA_SERVER_LOGIN" --password "$HTTP_DATA_SERVER_PWD" ${pub_url}
	echo "==> download complete, saved to file: '${pub}'"
}

cleandumps() {
	local old_dumps=$(find "${ALYX_DUMP_DIR}/sql/." -type f -mmin +$((${ALYX_DUMP_EXPIRES} * 60 * 24)) 2>/dev/null)
	if [[ ! -z ${old_dumps} ]]; then
		echo "=> cleaning sql dumps ${old_dumps}"
		rm -f ${old_dumps}
	else
		echo "=> no sql dumps to clean from $(ls -lh ${ALYX_DUMP_DIR}/sql)"
	fi

	# TODO: path in ONE params set to data
	# sudo find ~/IBL-pipeline/data/FlatIron -mindepth 1 ! -type d -not -name histology -delete
}

alyxcfg() {
	echo "# => configuring alyx"
	echo "# ==> checking if environment variables and databases exist"

	[ -z "$PGUSER" ] && err_exit "PGUSER environment variable is not set"
	[ -z "$PGPASSWORD" ] && err_exit "PGPASSWORD environment variable is not set"
	[ -z "$PGHOST" ] && err_exit "PGHOST environment variable is not set"
	[ -z "$ALYX_SECRET_KEY" ] && err_exit "ALYX_SECRET_KEY environment variable is not set"

	# custom settings_secret for multiple DBs
	echo "# ==> configuring settings_*.py"

	sed -i \
		-e "s/'localhost'/'localhost','$PGHOST'/" \
		"${IBL_PATH_ALYX}/alyx/alyx/settings_lab.py"

	sed -i \
		-e "s/%SECRET_KEY%/$ALYX_SECRET_KEY/" \
		-e "s/%DBNAME%/$PGDBNAME/" \
		-e "s/%DBUSER%/$PGUSER/" \
		-e "s/%DBPASSWORD%/$PGPASSWORD/" \
		-e "s/127.0.0.1/$PGHOST/" \
		"${IBL_PATH_ALYX}/alyx/alyx/settings_secret_template.py"

	sed \
		-e "s/%SECRET_KEY%/$ALYX_SECRET_KEY/" \
		-e "s/%DBNAME%/$PGDBNAME/" \
		-e "s/%DBUSER%/$PGUSER/" \
		-e "s/%DBPASSWORD%/$PGPASSWORD/" \
		-e "s/127.0.0.1/$PGHOST/" \
		>"${IBL_PATH_ALYX}/alyx/alyx/settings_secret.py" <<-EOF
			SECRET_KEY  = '%SECRET_KEY%'

			DATABASES  = {
			    'default': {
			        'ENGINE': 'django_prometheus.db.backends.postgresql',
			        'NAME': '%DBNAME%',
			        'USER': '%DBUSER%',
			        'PASSWORD': '%DBPASSWORD%',
			        'HOST': '127.0.0.1',
			        'PORT': '5432',
			    },
			    'old': {
			        'ENGINE': 'django_prometheus.db.backends.postgresql',
			        'NAME': '%DBNAME%_old',
			        'USER': '%DBUSER%',
			        'PASSWORD': '%DBPASSWORD%',
			        'HOST': '127.0.0.1',
			        'PORT': '5432',
			    }
			}

			EMAIL_HOST = 'mail.superserver.net'
			EMAIL_HOST_USER = 'alyx@awesomedomain.org'
			EMAIL_HOST_PASSWORD = 'UnbreakablePassword'
			EMAIL_PORT = 587
			EMAIL_USE_TLS = True
		EOF

	# '/home/joseph/.local/lib/python3.9/site-packages'
	sed -i \
		-e "s/alyx.internationalbrainlab.org/$PGHOST/" \
		"${IBL_PATH_ALYX}/docs/_static/001-alyx.conf"

	cat "${IBL_PATH_ALYX}/docs/_static/001-alyx.conf" >/etc/apache2/sites-available/001-alyx.conf

	install -m 660 /dev/null ~/.pgpass
	echo "${PGHOST}:5432:${PGDBNAME}:${PGUSER}:${PGPASSWORD}" >>~/.pgpass
}

alyxmigrate() {
	checkdb
	[[ ${DB_NAME_EXISTS} -eq 0 ]] &&
		err_exit "database '${PGDBNAME}' does not yet exist, run 'mkdb'"

	checkdbusr
	[[ ${DB_USER_EXISTS} -eq 0 ]] &&
		err_exit "database user '${PGUSER}' does not yet exist"

	echo "# ===> makemigrations"
	alyxcmd makemigrations

	echo "# ====> migrate"
	alyxcmd migrate

	if [ ! -f "$sucreated" ]; then
		set -e
		echo "# ==> creating alyx superuser"
		alyxcmd shell <<-EOF
			from misc.models import LabMember
			LabMember.objects.create(
				username='$PGUSER',
				password='$PGPASSWORD',
			    is_superuser=True,
				is_staff=True,
				email='joseph@datajoint.com')
		EOF
		touch ${sucreated}
		set +e
	fi
}

loaddb() {
	if [ -f "${dbloaded}" ]; then
		echo "# => database loaded - skipping load."
	else
		checkdb
		[[ ${DB_NAME_EXISTS} -eq 0 ]] &&
			err_exit "database '${PGDBNAME}' does not yet exist, run 'mkdb'"
		echo "# => resetting database"
		[ ! -f "${ALYX_DUMP_FILE}" ] && err_exit "database dump file '$ALYX_DUMP_FILE' doesn't exist"
		alyxcmd reset_db --noinput
		alyxcmd migrate
		echo "# ==> loading database ${ALYX_DUMP_FILE}"
		gzip -dc ${ALYX_DUMP_FILE} | psql -d ${PGDBNAME}
		alyxcmd migrate
		touch ${dbloaded}
	fi
}

renamedb() {
	echo "# => renaming databases:"

	checkdb "${PGDBNAME}_old"
	if [[ ${DB_NAME_EXISTS} -eq 1 ]]; then
		echo "# ==> dropping ${PGDBNAME}_old"
		dropdb "${PGDBNAME}_old" || err_exit "couldn't drop ${PGDBNAME}_old"
	fi

	echo "# ===> renaming ${PGDBNAME} to ${PGDBNAME}_old"
	psql -c "alter database ${PGDBNAME} rename to ${PGDBNAME}_old;" >/dev/null ||
		err_exit "couldn't rename ${PGDBNAME} to ${PGDBNAME}_old"

	echo "# ====> creating new ${PGDBNAME}"
	createdb ${PGDBNAME} || err_exit "couldn't create ${PGDBNAME} db"

	rm -f ${dbloaded}
	rm -f ${sucreated}

	echo "# ======> ok"
}

dumpjson() {
	if [ ! -f "${dbloaded}" ]; then
		echo "# => database not loaded - skipping json dump."
		return 0
	fi

	local json_dump_file="${ALYX_DUMP_DIR}/json/alyx_full.json"

	[[ -f "${json_dump_file}" ]] && mv -f "${json_dump_file}" "${json_dump_file}.last"

	echo "# => dumping db as JSON to ${json_dump_file}"
	alyxcmd dumpdata \
		-e contenttypes -e auth.permission \
		-e reversion.version -e reversion.revision -e admin.logentry \
		-e actions.ephyssession \
		-e actions.notification \
		-e actions.notificationrule \
		-e actions.virusinjection \
		-e data.download \
		-e experiments.brainregion \
		-e jobs.task \
		-e misc.note \
		-e subjects.subjectrequest \
		--indent 1 -o "${json_dump_file}"
}

dumpsql() {
	echo "# => performing sql backup"
	pg_dump -cOx -U ${PGUSER} -h ${PGHOST} -d ${PGDBNAME} -f "${ALYX_DUMP_DIR}/backup/alyx_full.sql"
	echo "# ==> zipping sql backup"
	gzip -f "${ALYX_DUMP_DIR}/backup/alyx_full.sql"
}

alyxstart() {
	echo "# => starting alyx"
	alyxcmd runserver --insecure 0.0.0.0:${ALYX_PORT}
}

# Routines
# --------

initdb() {
	mkdb
	alyxcfg
	alyxmigrate
}

loaddump() {
	fetchdump
	initdb
	loaddb
}

www() {
	loaddump
	alyxstart
}

reloaddb() {
	fetchdump
	rm -f ${dbloaded}
	loaddb
}

# Development
# -----------

dev() {
	echo -e "Starting development environment...\n"
	exec tail -f /dev/null
}

# Start
# -----

source "${VIRTUAL_ENV}/bin/activate"

# run sequence of operations specified by user
if [[ ${#OPS_SEQ[@]} -gt 0 ]]; then
	echo "# > operation sequence: ${OPS_SEQ[@]}"
	for fn in ${OPS_SEQ[@]}; do
		echo "---- ${fn}() ----"
		$fn
	done
fi

# run rest of user specified commands
exec "$@"
